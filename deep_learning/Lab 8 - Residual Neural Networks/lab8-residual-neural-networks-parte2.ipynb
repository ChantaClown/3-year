{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GxLyX9vdCnf"
   },
   "source": [
    "# Práctica 8: Residual neural networks - Parte 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjhho6Jb6-j"
   },
   "source": [
    "### Pre-requisitos. Instalar paquetes\n",
    "\n",
    "Para la segunda parte de este Laboratorio 8 necesitaremos TensorFlow y TensorFlow-Datasets. Además, como habitualmente, fijaremos la semilla aleatoria para asegurar la reproducibilidad de los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wjX2mh1-GSga"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 11:31:34.110557: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-08 11:31:34.114776: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-08 11:31:34.133223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731061894.166052   10489 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731061894.174011   10489 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 11:31:34.201177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Fijamos la semilla para poder reproducir los resultados\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "seed=1234\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyTV4jfNeiRk"
   },
   "source": [
    "Además, cargamos también APIs que vamos a emplear para que el código quede más legible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o3-nLMx-enlq"
   },
   "outputs": [],
   "source": [
    "#API de Keras, modelo Sequential y la capa Dense \n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "#Para mostrar gráficas\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd9GpQD5WVY4"
   },
   "source": [
    "### Carga del conjunto de datos\n",
    "\n",
    "De nuevo, seguimos empleando el conjunto *german_credit_numeric* ya empleado en los laboratorios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gpsZlN_mEj7D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731061916.771884   10489 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# TODO: Carga el conjunto german_credit como ds_train\n",
    "# Indica además un tamaño de batch de 128 y que se repita indefinidamente\n",
    "ds_train = tfds.load('german_credit_numeric', split='train',as_supervised=True).batch(128).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del desvanecimiento del gradiente\n",
    "En este apartado visualizaremos los tamaños de los gradientes, como hicimos en la primera parte. Para ello mantenemos la declaración de `GradientLoggingSequentialModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientLoggingSequentialModel(tf.keras.models.Sequential):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # En la inicialización instanciamos una nueva variable en la que\n",
    "        # registraremos el historial de tamaños de gradientes de cada capa\n",
    "        self.gradient_history = {}\n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        result = super().compile(**kwargs)\n",
    "        # Una vez sabemos la arquitectura, podemos inicializar la historia\n",
    "        # de gradientes de cada capa a una lista vacía.\n",
    "        for l in self.layers:\n",
    "            self.gradient_history[l.name] = []\n",
    "        return result\n",
    "        \n",
    "    def _save_gradients(self, gradients):\n",
    "        # A cada paso de entrenamiento llamaremos a esta función para que\n",
    "        # registre los gradientes.\n",
    "        # En la lista gradients se encuentran los gradientes de las distintas\n",
    "        # capas por orden. Cada capa l tendrá un número de gradientes que\n",
    "        # concidirá con l.trainable_variables.\n",
    "        # Teniendo esto en cuenta, recorremos los gradientes, calculamos su\n",
    "        # tamaño y guardamos la media de tamaños de cada capa en el histórico\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            gradient_sizes = []\n",
    "            for lw in layer.trainable_variables:\n",
    "                g_size = np.linalg.norm(gradients[i].numpy())\n",
    "                gradient_sizes.append(g_size)\n",
    "                i += 1\n",
    "            mean_gradient_size = np.mean(gradient_sizes)\n",
    "            self.gradient_history[layer.name].append(mean_gradient_size)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Haremos un paso de entrenamiento personalizado basado en \n",
    "        # https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example\n",
    "        # Dejaremos el ejemplo tal cual, añadiendo tan solo la llamada a\n",
    "        # _save_gradients una vez que disponemos de los gradientes\n",
    "        \n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Llamada añadida para grabar los gradientes.\n",
    "        self._save_gradients(gradients)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del bloque con conexión residual\n",
    "\n",
    "En una red neuronal *feed-forward* convencional, la salida de cada capa se utiliza como entrada de las capa siguiente.\n",
    "<img src=\"./img/resnet.png\" alt=\"Dos capas en una red FF\" width=\"400\"/>\n",
    "\n",
    "En contraste, en una ResNet se introducen bloques que incluyen conexiones residuales con el objetivo de favorecer la propagación de los gradientes.\n",
    "<img src=\"./img/resnet-2.png\" alt=\"Dos capas en una red FF con conexión residual\" width=\"400\"/>\n",
    "\n",
    "A pesar de que las ResNet se suelen utilizar con redes convolucionales, en este Laboratorio utilizaremos una red *feed-forward*.\n",
    "\n",
    "Para poder utilizar este tipo de bloques en nuestra arquitectura definiremos un nuevo tipo de modelo denominado `DoubleDenseWithSkipModel` que consistirá en lo representado en la imagen:\n",
    " - Una primera capa Dense, cuya salida sirve de entrada a...\n",
    " - Una segunda capa Dense lineal (sin activación), cuya salida sirve de entrada a...\n",
    " - A una operación suma que la añade a la entrada original a la primera capa. La salida de esta suma servirá de entrada a...\n",
    " - Una función de activación, cuya salida será la salida del bloque.\n",
    "\n",
    "Utilizaremos la función sigmoide como función de activación en ambos casos. La entrada y la salida del bloque deberán tener la misma dimensión para que se pueda realizar la suma. Por simplicidad, mantendremos la salida de la primera capa con la misma dimensión.\n",
    "\n",
    "Nuestra nueva clase heredará de `Model`, por lo que deberá implementar los métodos `build` y `call`. En celdas posteriores añadiremos estos bloques a un modelo `Sequential` como si fuesen capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDenseWithSkipModel(tf.keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DoubleDenseWithSkipModel, self).__init__(**kwargs)\n",
    "\n",
    "    # TODO: Completa el método build\n",
    "    def build(self, input_shape):\n",
    "        ...\n",
    "        \n",
    "    # TODO: Completa el método skip\n",
    "    def call(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENqWLaw7fuxl"
   },
   "source": [
    "## Creamos un modelo *GradientLoggingSequentialModel* utilizando las nuevas capas\n",
    "Creamos un modelo *GradientLoggingSequentialModel* para ajustar a los datos de entrada siguiendo las especificaciones dadas. Deberá incluir (además de las capas de entrada y salida) una capa de 10 unidades con activación sigmoide y 10 de los nuevos bloques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrIOT-4DmRuu",
    "outputId": "79dba569-e5e5-426f-9a87-57757db7d363"
   },
   "outputs": [],
   "source": [
    "# TODO - Define en model una red GradientLoggingSequentialModel\n",
    "# Pon una capa densa con 10 unidades con activación sigmoide y 10 capas DoubleDenseWithSkipModel\n",
    "model = ...\n",
    "\n",
    "#Construimos el modelo y mostramos \n",
    "model.build()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Como en la primera parte, vamos a establecer la función de pérdida (entropía cruzada binaria), el optimizador (SGD con LR $10^{-3}$) y la métrica que nos servirá para evaluar el rendimiento del modelo entrenado (área bajo la curva)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Compila el modelo. Utiliza la opción run_eagerly=True para que se puedan registrar los gradientes a cada paso\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx2nvo6G0Qvn"
   },
   "source": [
    "Entrenamos el modelo usando model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EzQDyKfGfuu"
   },
   "outputs": [],
   "source": [
    "#TODO - entrenar el modelo utilizando 8 steps por epoch. Con 10 epochs nos valdrá para comprobar el desvanecimiento de gradientes.\n",
    "history = model.fit(ds_train, epochs=10, steps_per_epoch=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcjua_KlWymG"
   },
   "source": [
    "Una vez hayas encontrado un valor de learning rate que consiga una convergencia rápida, guarda el history de la pérdida en la variable history_sgd para poder hacer comparativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(14, 6), dpi=80)\n",
    "pyplot.boxplot(model.gradient_history.values())\n",
    "pyplot.yscale('log')\n",
    "pyplot.xticks(ticks=range(1,len(model.gradient_history)+1), labels=model.gradient_history.keys())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa\n",
    " - Contrasta los resultados con los obtenidos en la primera parte\n",
    " - Cambia las activaciones del bloque y de la primera capa oculta de la red a ReLU y observa la diferencia.\n",
    " - Alarga el entrenamiento y prueba distintos optimizadores para intentar que el modelo entrene correct"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_parte1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
